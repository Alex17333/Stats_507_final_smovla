{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61faef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import pprint\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db488013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\19142\\.conda\\envs\\my_robotics\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from lerobot.datasets.lerobot_dataset import LeRobotDataset\n",
    "from lerobot.policies.factory import make_policy\n",
    "from lerobot.policies.smolvla.configuration_smolvla import SmolVLAConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c8f55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bff48c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'torchcodec' is not available in your platform, falling back to 'pyav' as a default decoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 19631 frames\n",
      "{'count': array([19631]),\n",
      " 'max': array([ 72.7734375 , 179.47265625, 164.61914062,  96.59179688,\n",
      "       123.57421875,  34.9307785 ]),\n",
      " 'mean': array([ 14.51138199, 146.44867041, 143.31572513,  62.96079529,\n",
      "        85.83100241,   7.78159506]),\n",
      " 'min': array([-37.17773438,  48.8671875 ,  40.95703125,   9.66796875,\n",
      "        56.25      ,   0.        ]),\n",
      " 'std': array([27.98693199, 34.98953716, 21.46425995, 16.91135693, 12.47836367,\n",
      "        9.54603304])}\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"lerobot/svla_so100_pickplace\"\n",
    "\n",
    "fps = 30\n",
    "\n",
    "dataset = LeRobotDataset(\n",
    "    repo_id, \n",
    "    episodes=[0],\n",
    "    delta_timestamps={\"action\": [i / fps for i in range(10)]}\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(dataset)} frames\")\n",
    "\n",
    "pprint.pprint(dataset.meta.stats[\"action\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d9ac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lerobot.configs.policies:Device 'None' is not available. Switching to 'cuda'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Policy...\n",
      "Reducing the number of VLM layers to 16 ...\n"
     ]
    }
   ],
   "source": [
    "policy_cfg = SmolVLAConfig(\n",
    "    n_action_steps=10,\n",
    "    chunk_size=10,\n",
    "    pretrained_path=\"Jill111/my_smolvla\",\n",
    "    empty_cameras=0\n",
    ")\n",
    "\n",
    "print(\"Building Policy...\")\n",
    "policy = make_policy(\n",
    "    cfg=policy_cfg,\n",
    "    ds_meta=dataset.meta\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41e501c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "policy.to(device)\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6757254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_path = \"HuggingFaceTB/SmolVLM-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90d8fead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\19142\\.conda\\envs\\my_robotics\\lib\\site-packages\\torchvision\\io\\_video_deprecation_warning.py:5: UserWarning: The video decoding and encoding capabilities of torchvision are deprecated from version 0.22 and will be removed in version 0.24. We recommend that you migrate to TorchCodec, where we'll consolidate the future decoding/encoding capabilities of PyTorch: https://github.com/pytorch/torchcodec\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pick up the cube and place it in the box.\n",
      "Text tokenized shape: torch.Size([1, 64])\n",
      "Lang Mask Shape:   torch.Size([1, 64])\n",
      "Batch keys: dict_keys(['observation.images.top', 'observation.images.wrist', 'action', 'observation.state', 'timestamp', 'frame_index', 'episode_index', 'index', 'task_index', 'action_is_pad', 'observation.language.tokens', 'observation.language.attention_mask'])\n",
      "Image shape: torch.Size([1, 3, 480, 640])\n",
      "Action shape: torch.Size([1, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "item = dataset[0]\n",
    "print(item[\"task\"])\n",
    "\n",
    "encoded_text = tokenizer( \n",
    "    item[\"task\"],\n",
    "    padding=\"max_length\",\n",
    "    max_length=64,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# print(item.items())\n",
    "batch = {k: v.unsqueeze(0).to(device) for k, v in item.items() if isinstance(v, torch.Tensor)}\n",
    "\n",
    "batch[\"observation.language.tokens\"] = encoded_text[\"input_ids\"].to(device)\n",
    "batch[\"observation.language.attention_mask\"] = encoded_text[\"attention_mask\"].to(device).bool()\n",
    "\n",
    "print(\"Text tokenized shape:\", batch[\"observation.language.tokens\"].shape)\n",
    "print(\"Lang Mask Shape:  \", batch[\"observation.language.attention_mask\"].shape)\n",
    "print(\"Batch keys:\", batch.keys())\n",
    "print(f\"Image shape: {batch['observation.images.top'].shape}\")\n",
    "print(f\"Action shape: {batch['action'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02252435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "\n",
    "def normalize_data(data, mean, std):\n",
    "    return (data - mean) / std\n",
    "\n",
    "action_mean = torch.from_numpy(dataset.meta.stats[\"action\"][\"mean\"]).float().to(device)\n",
    "action_std = torch.from_numpy(dataset.meta.stats[\"action\"][\"std\"]).float().to(device)\n",
    "\n",
    "# Convert the data in the batch to float 32\n",
    "\n",
    "batch[\"action\"] = batch[\"action\"].float()\n",
    "if \"observation.state\" in batch:\n",
    "    batch[\"observation.state\"] = batch[\"observation.state\"].float()\n",
    "\n",
    "batch[\"action\"] = normalize_data(batch[\"action\"], action_mean, action_std)\n",
    "\n",
    "if \"observation.state\" in batch:\n",
    "    state_mean = torch.from_numpy(dataset.meta.stats[\"observation.state\"][\"mean\"]).float().to(device)\n",
    "    state_std = torch.from_numpy(dataset.meta.stats[\"observation.state\"][\"std\"]).float().to(device)\n",
    "    batch[\"observation.state\"] = normalize_data(batch[\"observation.state\"], state_mean, state_std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "857b7e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.029459310695528984\n",
      "Output dict keys: dict_keys(['losses_after_forward', 'losses_after_rm_padding', 'loss'])\n"
     ]
    }
   ],
   "source": [
    "policy.train()\n",
    "\n",
    "loss, output_dict = policy.forward(batch)\n",
    "\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "print(\"Output dict keys:\", output_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f999d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Action Shape: torch.Size([1, 6])\n",
      "Action sample: tensor([-0.3268,  0.8102], device='cuda:0')\n",
      "✅ Action looks valid (numerical-wise).\n"
     ]
    }
   ],
   "source": [
    "policy.eval() \n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    action = policy.select_action(batch)\n",
    "\n",
    "print(f\"Generated Action Shape: {action.shape}\") \n",
    "\n",
    "print(\"Action sample:\", action[0, :2]) \n",
    "if torch.isnan(action).any():\n",
    "    print(\"❌ ALERT: Action contains NaN!\")\n",
    "else:\n",
    "    print(\"✅ Action looks valid (numerical-wise).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f232018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gpu(data):\n",
    "    if isinstance(data, dict):\n",
    "        return {k: to_gpu(v) for k, v in data.items()}\n",
    "    elif isinstance(data, list):\n",
    "        return [to_gpu(i) for i in data]\n",
    "    elif isinstance(data, tuple):\n",
    "        return tuple([to_gpu(i) for i in data])\n",
    "    elif torch.is_tensor(data) and data.device != device:\n",
    "        return data.to(device)\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac20af42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start dummy training loop...\n",
      "Step 0, Loss: 0.0316\n",
      "Step 1, Loss: 0.5477\n",
      "Step 2, Loss: 0.1490\n",
      "Step 3, Loss: 0.0552\n",
      "Step 4, Loss: 0.2580\n",
      "Step 5, Loss: 0.0783\n",
      "Step 6, Loss: 0.1517\n",
      "Step 7, Loss: 0.1570\n",
      "Step 8, Loss: 0.2996\n",
      "Step 9, Loss: 0.1385\n",
      "Step 10, Loss: 0.1314\n",
      "Step 11, Loss: 0.1249\n",
      "Step 12, Loss: 0.1230\n",
      "Step 13, Loss: 0.1000\n",
      "Step 14, Loss: 0.2439\n",
      "Step 15, Loss: 0.0629\n",
      "Step 16, Loss: 0.2381\n",
      "Step 17, Loss: 0.3264\n",
      "Step 18, Loss: 0.1327\n",
      "Step 19, Loss: 0.1289\n",
      "Step 20, Loss: 0.1141\n",
      "Step 21, Loss: 0.1663\n",
      "Step 22, Loss: 0.1360\n",
      "Step 23, Loss: 0.1040\n",
      "Step 24, Loss: 0.0727\n",
      "Step 25, Loss: 0.0933\n",
      "Step 26, Loss: 0.0916\n",
      "Step 27, Loss: 0.0629\n",
      "Step 28, Loss: 0.0790\n",
      "Step 29, Loss: 0.0946\n",
      "Step 30, Loss: 0.0396\n",
      "Step 31, Loss: 0.1032\n",
      "Step 32, Loss: 0.0767\n",
      "Step 33, Loss: 0.0609\n",
      "Step 34, Loss: 0.0544\n",
      "Step 35, Loss: 0.0728\n",
      "Step 36, Loss: 0.0552\n",
      "Step 37, Loss: 0.1494\n",
      "Step 38, Loss: 0.0541\n",
      "Step 39, Loss: 0.0531\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "policy = policy.to(device)\n",
    "batch = to_gpu(batch) \n",
    "\n",
    "optimizer = torch.optim.AdamW(policy.parameters(), lr=1e-4)\n",
    "policy.train()\n",
    "\n",
    "print(\"Start dummy training loop...\")\n",
    "\n",
    "loss_list = []\n",
    "steps = []\n",
    "\n",
    "for i in range(40):\n",
    "    \n",
    "    loss, _ = policy.forward(batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_list.append(loss.item())\n",
    "    steps.append(i)\n",
    "\n",
    "    print(f\"Step {i}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1f7269c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mstep\u001b[49m, loss_list)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'step' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(step, loss_list)\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_robotics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
